{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation(2).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate models on testset"
      ],
      "metadata": {
        "id": "uBNWscSLK5_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Iq72QT_kKyIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43a633b-e054-477f-eb2c-fcade7def04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "dZlX2U4EVATI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_from_disk, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "model_checkpoint = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PQA/\"\n",
        "max_length = 512 # The maximum length of a feature (question and context)\n",
        "doc_stride = 256 # The authorized overlap between two part of the context when splitting it is needed."
      ],
      "metadata": {
        "id": "618rOPEDVZm5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare Test set"
      ],
      "metadata": {
        "id": "Wg-WTl4PaRV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_from_disk(DRIVE_PATH + \"test2.hf\").shuffle(seed=42)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "1UJ7iK41VTz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cd9a1c-1bc7-4357-c475-1138fb8e5346"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/drive/MyDrive/PQA/test2.hf/cache-a8e9793b8e562ab4.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EM and F1 and BLEU parameters "
      ],
      "metadata": {
        "id": "078NSxVOag-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(prediction, answer):\n",
        "    pred_tokens = prediction.split()\n",
        "    answer_tokens = answer.split()\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(answer_tokens) == 0:\n",
        "        return int(pred_tokens == answer_tokens)\n",
        "    \n",
        "    common_tokens = set(pred_tokens) & set(answer_tokens)\n",
        "    \n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(answer_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def compute_exact_match(prediction, answer):\n",
        "    return int(prediction == answer)\n",
        "\n",
        "def bleu(prediction, answer) : \n",
        "  reference = [answer.split(' ')]\n",
        "  candidate = pred.split(' ')\n",
        "  BLEU = sentence_bleu(reference, candidate)\n",
        "  BLEU1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "  BLEU4 = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))\n",
        "\n",
        "  return BLEU, BLEU1, BLEU4\n"
      ],
      "metadata": {
        "id": "MrkQ-MJ9Wx2Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Class"
      ],
      "metadata": {
        "id": "HDvijV8JaelA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerPredictor:\n",
        "  def __init__(self, model, tokenizer, device='cuda', n_best=10, max_length=512, stride=256, no_answer=False):\n",
        "      \"\"\"Initializes PyTorch Question Answering Prediction\n",
        "      It's best to leave use the default values.\n",
        "      Args:\n",
        "          model: Fine-tuned torch model\n",
        "          tokenizer: Transformers tokenizer\n",
        "          device (torch.device): Running device\n",
        "          n_best (int): Number of best possible answers\n",
        "          max_length (int): Tokenizer max length\n",
        "          stride (int): Tokenizer stride\n",
        "          no_answer (bool): If True, model can return \"no answer\"\n",
        "      \"\"\"\n",
        "      self.model = model.eval().to(device)\n",
        "      self.tokenizer = tokenizer\n",
        "      self.device = device\n",
        "      self.max_length = max_length\n",
        "      self.stride = stride\n",
        "      self.no_answer = no_answer\n",
        "      self.n_best = n_best\n",
        "\n",
        "\n",
        "  def model_pred(self, questions, contexts, batch_size=1):\n",
        "      n = len(contexts)\n",
        "      if n%batch_size!=0:\n",
        "          raise Exception(\"batch_size must be divisible by sample length\")\n",
        "\n",
        "      tokens = self.tokenizer(questions, contexts, add_special_tokens=True, \n",
        "                              return_token_type_ids=True, return_tensors=\"pt\", padding=True, \n",
        "                              return_offsets_mapping=True, truncation=\"only_second\", \n",
        "                              max_length=self.max_length, stride=self.stride)\n",
        "\n",
        "      start_logits, end_logits = [], []\n",
        "      for i in tqdm(range(0, n-batch_size+1, batch_size)):\n",
        "          with torch.no_grad():\n",
        "              out = self.model(tokens['input_ids'][i:i+batch_size].to(self.device), \n",
        "                          tokens['attention_mask'][i:i+batch_size].to(self.device), \n",
        "                          tokens['token_type_ids'][i:i+batch_size].to(self.device))\n",
        "\n",
        "              start_logits.append(out.start_logits)\n",
        "              end_logits.append(out.end_logits)\n",
        "\n",
        "      return tokens, torch.stack(start_logits).view(n, -1), torch.stack(end_logits).view(n, -1)\n",
        "\n",
        "\n",
        "  def __call__(self, questions, contexts, batch_size=1, answer_max_len=100):\n",
        "      \"\"\"Creates model prediction\n",
        "      \n",
        "      Args: \n",
        "          questions (list): Question strings\n",
        "          contexts (list): Contexts strings\n",
        "          batch_size (int): Batch size\n",
        "          answer_max_len (int): Sets the longests possible length for any answer\n",
        "        \n",
        "      Returns:\n",
        "          dict: The best prediction of the model\n",
        "              (e.g {0: {\"text\": str, \"score\": int}})\n",
        "      \"\"\"\n",
        "      tokens, starts, ends = self.model_pred(questions, contexts, batch_size=batch_size)\n",
        "      start_indexes = starts.argsort(dim=-1, descending=True)[:, :self.n_best]\n",
        "      end_indexes = ends.argsort(dim=-1, descending=True)[:, :self.n_best]\n",
        "\n",
        "      preds = {}\n",
        "      for i, (c, q) in enumerate(zip(contexts, questions)):  \n",
        "          min_null_score = starts[i][0] + ends[i][0] # 0 is CLS Token\n",
        "          start_context = tokens['input_ids'][i].tolist().index(self.tokenizer.sep_token_id)\n",
        "          \n",
        "          offset = tokens['offset_mapping'][i]\n",
        "          valid_answers = []\n",
        "          for start_index in start_indexes[i]:\n",
        "              # Don't consider answers that are in questions\n",
        "              if start_index<start_context:\n",
        "                  continue\n",
        "              for end_index in end_indexes[i]:\n",
        "                  # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                  # to part of the input_ids that are not in the context.\n",
        "                  if (start_index >= len(offset) or end_index >= len(offset)\n",
        "                      or offset[start_index] is None or offset[end_index] is None):\n",
        "                      continue\n",
        "                  # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                  if end_index < start_index or (end_index-start_index+1) > answer_max_len:\n",
        "                      continue\n",
        "\n",
        "                  start_char = offset[start_index][0]\n",
        "                  end_char = offset[end_index][1]\n",
        "                  valid_answers.append({\"score\": (starts[i][start_index] + ends[i][end_index]).item(),\n",
        "                                        \"text\": c[start_char: end_char]})\n",
        "                  \n",
        "          if len(valid_answers) > 0:\n",
        "              best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "          else:\n",
        "              best_answer = {\"text\": \"\", \"score\": min_null_score}\n",
        "\n",
        "          if self.no_answer:\n",
        "              preds[i] = best_answer if best_answer[\"score\"] >= min_null_score else {\"text\": \"\", \"score\": min_null_score}\n",
        "          else:\n",
        "              preds[i] = best_answer\n",
        "\n",
        "      return preds\n"
      ],
      "metadata": {
        "id": "2VGmFe8AWas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ParsBERT Evaluation"
      ],
      "metadata": {
        "id": "_1nc6kb4aaKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/PQA/\"\n",
        "model_path = DRIVE_PATH + f\"checkpoints/checkpoint-2548/\"  ## load model trained for 2 epochs\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "2KBZjvX1WYYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = AnswerPredictor(model, tokenizer, device='cuda', n_best=10, no_answer=True)\n",
        "\n",
        "EH = F1 = BLEU = BLEU1 = BLEU4 = 0\n",
        "\n",
        "for example in test_dataset: \n",
        "\n",
        "  if example['answers']['text'] == [] :\n",
        "    context = example['context'] \n",
        "    question = example['question'] \n",
        "    preds = predictor([question], [context], batch_size=1)\n",
        "    pred = preds[0]['text'].strip()\n",
        "    if len(pred) == 0 : \n",
        "      EH += 1\n",
        "      F1 += 1\n",
        "      BLEU += 1\n",
        "      BLEU1 += 1\n",
        "      BLEU4 += 1\n",
        "\n",
        "    continue\n",
        "\n",
        "  context = example['context'] \n",
        "  question = example['question'] \n",
        "  preds = predictor([question], [context], batch_size=1)\n",
        "  pred = preds[0]['text'].strip()\n",
        "\n",
        "  ######### find best match #######################\n",
        "  index = 0\n",
        "  max_score = 0\n",
        "  for m in range(len(example['answers']['text'])) : \n",
        "    temp_ans = example['answers']['text'][m]\n",
        "    temp_f1 = compute_f1(pred, temp_ans)\n",
        "    if temp_f1 > max_score : \n",
        "      index = m\n",
        "      max_score = temp_f1\n",
        "  ##################################################\n",
        "\n",
        "  answer = example['answers']['text'][index]\n",
        "\n",
        "\n",
        "  EH += compute_exact_match(pred, answer)\n",
        "  F1 += compute_f1(pred, answer)\n",
        "  b, b1, b4 = bleu(pred, answer)\n",
        "  BLEU += b\n",
        "  BLEU1 += b1\n",
        "  BLEU4 += b4\n",
        "\n",
        "EH /= len(test_dataset)\n",
        "F1 /= len(test_dataset)\n",
        "BLEU /= len(test_dataset)\n",
        "BLEU1 /= len(test_dataset)\n",
        "BLEU4 /= len(test_dataset)"
      ],
      "metadata": {
        "id": "vqqFef7ZW1ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Exact match of Pars BERT on testset : ' + str(EH))\n",
        "print('F1 score of Pars BERT on testset : ' + str(F1))\n",
        "print('BLEU score of Pars BERT on testset : ' + str(BLEU))\n",
        "print('BLEU1 score of Pars BERT on testset : ' + str(BLEU1))\n",
        "print('BLEU4 score of Pars BERT on testset : ' + str(BLEU4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4lDEg0Scz_Q",
        "outputId": "21a9aab0-5e85-47f4-82a9-7272720e212f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match of Pars BERT on testset : 0.46153846153846156\n",
            "F1 score of Pars BERT on testset : 0.6146364364558041\n",
            "BLEU score of Pars BERT on testset : 0.4131725420582979\n",
            "BLEU1 score of Pars BERT on testset : 0.5793761765330291\n",
            "BLEU4 score of Pars BERT on testset : 0.4074068024567045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of ParsT5"
      ],
      "metadata": {
        "id": "oyzFHwIQLvyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown 1Lcs5eGTIhy0JUY9FW2pn-80m3CHyVtvQ\n",
        "! unzip ParsT5.zip\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "TH-VTc5ULvIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "model_path = '/content/content/drive/MyDrive/parsT5_QA/model_4'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "ymSuoRLKMNkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EH = F1 = BLEU = BLEU1 = BLEU4 \n",
        "\n",
        "for example in test_dataset: \n",
        "\n",
        "  context = example['context'] \n",
        "  question = example['question']\n",
        "  \n",
        "  input = 'متن: ' + context + '، پرسش: ' + question\n",
        "  input_ids = tokenizer.encode(input, return_tensors='pt')\n",
        "  output_ids = model.generate(input_ids, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "  output = ' '.join([tokenizer.decode(id) for id in output_ids])\n",
        "  pred = output.replace('<pad>', '').replace('</s>', '').strip()\n",
        "\n",
        "  \n",
        "  if example['answers']['text'] == [] :\n",
        "    if pred == 'بدون پاسخ' : \n",
        "      EH += 1\n",
        "      F1 += 1\n",
        "      BLEU += 1\n",
        "      BLEU1 += 1\n",
        "      BLEU4 += 1\n",
        "    continue\n",
        "\n",
        "   ######### find best match #######################\n",
        "  index = 0\n",
        "  max_score = 0\n",
        "  for m in range(len(example['answers']['text'])) : \n",
        "    temp_ans = example['answers']['text'][m]\n",
        "    temp_f1 = compute_f1(pred, temp_ans)\n",
        "    if temp_f1 > max_score : \n",
        "      index = m\n",
        "      max_score = temp_f1\n",
        "  ##################################################\n",
        "\n",
        "  answer = example['answers']['text'][index]\n",
        "\n",
        "\n",
        "  EH += compute_exact_match(pred, answer)\n",
        "  F1 += compute_f1(pred, answer)\n",
        "  b, b1, b4 = bleu(pred, answer)\n",
        "  BLEU += b\n",
        "  BLEU1 += b1\n",
        "  BLEU4 += b4\n",
        "\n",
        "EH /= len(test_dataset)\n",
        "F1 /= len(test_dataset)\n",
        "BLEU /= len(test_dataset)\n",
        "BLEU1 /= len(test_dataset)\n",
        "BLEU4 /= len(test_dataset)"
      ],
      "metadata": {
        "id": "ZnPY4taiMRrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Exact match of ParsT5 on testset : ' + str(EH))\n",
        "print('F1 score of ParsT5 on testset : ' + str(F1))\n",
        "print('BLEU score of ParsT5 on testset : ' + str(BLEU))\n",
        "print('BLEU1 score of ParsT5 on testset : ' + str(BLEU1))\n",
        "print('BLEU4 score of ParsT5 on testset : ' + str(BLEU4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6ovVSKJPsO1",
        "outputId": "412ed6b2-501a-4897-9201-db97fe4a5c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match of ParsT5 on testset : 0.45327524038461536\n",
            "F1 score of ParsT5 on testset : 0.4815071358518649\n",
            "BLEU score of ParsT5 on testset : 0.3314958137059677\n",
            "BLEU1 score of ParsT5 on testset : 0.47223412366086\n",
            "BLEU4 score of ParsT5 on testset : 0.33133417034113266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALBERT Evaluation"
      ],
      "metadata": {
        "id": "GuFKhiPJy12r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/PQA/checkpoints/model/\"\n",
        "model_path = DRIVE_PATH + f\"checkpoint-1699/\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "device = 'cuda'\n",
        "model.eval().to(device)"
      ],
      "metadata": {
        "id": "MRJZek3Dy4hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = AnswerPredictor(model, tokenizer, device='cuda', n_best=10, no_answer=True)\n",
        "\n",
        "EH = F1 = BLEU = BLEU1 = BLEU4 = 0\n",
        "\n",
        "for example in test_dataset: \n",
        "\n",
        "  if example['answers']['text'] == [] :\n",
        "    context = example['context'] \n",
        "    question = example['question'] \n",
        "    preds = predictor([question], [context], batch_size=1)\n",
        "    pred = preds[0]['text'].strip()\n",
        "    if len(pred) == 0 : \n",
        "      EH += 1\n",
        "      F1 += 1\n",
        "      BLEU += 1\n",
        "      BLEU1 += 1\n",
        "      BLEU4 += 1\n",
        "\n",
        "    continue\n",
        "\n",
        "  context = example['context'] \n",
        "  question = example['question'] \n",
        "  preds = predictor([question], [context], batch_size=1)\n",
        "  pred = preds[0]['text'].strip()\n",
        "\n",
        "  ######### find best match #######################\n",
        "  index = 0\n",
        "  max_score = 0\n",
        "  for m in range(len(example['answers']['text'])) : \n",
        "    temp_ans = example['answers']['text'][m]\n",
        "    temp_f1 = compute_f1(pred, temp_ans)\n",
        "    if temp_f1 > max_score : \n",
        "      index = m\n",
        "      max_score = temp_f1\n",
        "  ##################################################\n",
        "\n",
        "  answer = example['answers']['text'][index]\n",
        "\n",
        "\n",
        "  EH += compute_exact_match(pred, answer)\n",
        "  F1 += compute_f1(pred, answer)\n",
        "  b, b1, b4 = bleu(pred, answer)\n",
        "  BLEU += b\n",
        "  BLEU1 += b1\n",
        "  BLEU4 += b4\n",
        "\n",
        "EH /= len(test_dataset)\n",
        "F1 /= len(test_dataset)\n",
        "BLEU /= len(test_dataset)\n",
        "BLEU1 /= len(test_dataset)\n",
        "BLEU4 /= len(test_dataset)"
      ],
      "metadata": {
        "id": "xfsqKNx3zh-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Exact match of ALBERT on testset : ' + str(EH))\n",
        "print('F1 score of ALBERT on testset : ' + str(F1))\n",
        "print('BLEU score of ALBERT on testset : ' + str(BLEU))\n",
        "print('BLEU1 score of ALBERT on testset : ' + str(BLEU1))\n",
        "print('BLEU4 score of ALBERT on testset : ' + str(BLEU4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9wlQZPi7QiB",
        "outputId": "62e871bc-acea-4839-aa62-5ab897f370d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match of ALBERT on testset : 0.4737079326923077\n",
            "F1 score of ALBERT on testset : 0.5291084943589603\n",
            "BLEU score of ALBERT on testset : 0.3697263492063661\n",
            "BLEU1 score of ALBERT on testset : 0.5156283202327573\n",
            "BLEU4 score of ALBERT on testset : 0.3680442027180467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mBERT Evaluation"
      ],
      "metadata": {
        "id": "3E86691h_aJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/PQA/\"\n",
        "model_path = DRIVE_PATH + f\"checkpoints/checkpoint-17000/\"  ## load model trained for 2 epochs\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "ZBkvgBq9_c87"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = AnswerPredictor(model, tokenizer, device='cuda', n_best=10, no_answer=True)\n",
        "\n",
        "EH = F1 = BLEU = BLEU1 = BLEU4 = 0\n",
        "cnt = 0\n",
        "for example in test_dataset: \n",
        "  cnt+=1\n",
        "  print(cnt)\n",
        "\n",
        "  if example['answers']['text'] == [] :\n",
        "    context = example['context'] \n",
        "    question = example['question'] \n",
        "    preds = predictor([question], [context], batch_size=1)\n",
        "    pred = preds[0]['text'].strip()\n",
        "    if len(pred) == 0 : \n",
        "      EH += 1\n",
        "      F1 += 1\n",
        "      BLEU += 1\n",
        "      BLEU1 += 1\n",
        "      BLEU4 += 1\n",
        "\n",
        "    continue\n",
        "\n",
        "  context = example['context'] \n",
        "  question = example['question'] \n",
        "  preds = predictor([question], [context], batch_size=1)\n",
        "  pred = preds[0]['text'].strip()\n",
        "\n",
        "  ######### find best match #######################\n",
        "  index = 0\n",
        "  max_score = 0\n",
        "  for m in range(len(example['answers']['text'])) : \n",
        "    temp_ans = example['answers']['text'][m]\n",
        "    temp_f1 = compute_f1(pred, temp_ans)\n",
        "    if temp_f1 > max_score : \n",
        "      index = m\n",
        "      max_score = temp_f1\n",
        "  ##################################################\n",
        "\n",
        "  answer = example['answers']['text'][index]\n",
        "\n",
        "\n",
        "  EH += compute_exact_match(pred, answer)\n",
        "  F1 += compute_f1(pred, answer)\n",
        "  b, b1, b4 = bleu(pred, answer)\n",
        "  BLEU += b\n",
        "  BLEU1 += b1\n",
        "  BLEU4 += b4\n",
        "\n",
        "EH /= len(test_dataset)\n",
        "F1 /= len(test_dataset)\n",
        "BLEU /= len(test_dataset)\n",
        "BLEU1 /= len(test_dataset)\n",
        "BLEU4 /= len(test_dataset)"
      ],
      "metadata": {
        "id": "vBSfyVeJ_xHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Exact match of mBERT on testset : ' + str(EH))\n",
        "print('F1 score of mBERT on testset : ' + str(F1))\n",
        "print('BLEU score of mBERT on testset : ' + str(BLEU))\n",
        "print('BLEU1 score of mBERT on testset : ' + str(BLEU1))\n",
        "print('BLEU4 score of mBERT on testset : ' + str(BLEU4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJ4F-1I_3HK",
        "outputId": "6346c306-db77-4936-d811-37f84abb3687"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match of mBERT on testset : 0.6484375\n",
            "F1 score of mBERT on testset : 0.6926880638353844\n",
            "BLEU score of mBERT on testset : 0.42744129904426637\n",
            "BLEU1 score of mBERT on testset : 0.6832573296203045\n",
            "BLEU4 score of mBERT on testset : 0.4261544478449596\n"
          ]
        }
      ]
    }
  ]
}
